{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253a3d76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Q-leraning, we maintain a matrix to save all the action values. However, this does not work for continuous space, or a complex environment with huge action and state spaces. It is natural to consider replace the Q-table by a function. In other words, we use a function to approximate Q. This is the core idea of DQN (Deep-Q Network).\n",
    "\n",
    "Notice that DQN may not be suitable for continuous action space, since we will do $\\max Q$ for the value updates, which could be hard for an NN if the actions are not discrete. \n",
    "\n",
    "First we write out the update rulws for Q-learning: $$Q(s, a) \\leftarrow Q(s, a) + \\alpha (r + \\gamma \\max_{a'}Q(s', a')- Q(s, a))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e50ab",
   "metadata": {},
   "source": [
    "# CartPole env\n",
    "In the CartPole Env, there exists a cart, on top of which stands a pole. The agent is required to move the cart horizontally such to make the pole standing. If the pole tilts too much, or the cart moves too far from the initial place, or neither of the two happenes for 200 frames, the game ends. The state of the agent is a tuple of (cart_position, cart_velocity, pole_angle, pole_tip_velocity). We have 2 actions, 0 for left and 1 for right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc127295",
   "metadata": {},
   "source": [
    "# DQN for CartPole env\n",
    "\n",
    "Instead of learning a function $f: (s, a) \\rightarrow Q$, we can simply set it to learn the action given the state, i.e. $f: s \\rightarrow a$. \n",
    "\n",
    "Now an improtant question comes to us: how do we define the loss function?\n",
    "\n",
    "It is simple to use an MSE loss between $Q_\\omega(s, a)$ (the learnt action value) and the TD estimation $r + \\gamma \\max_{a'}Q(s', a')$, which theoretically should equal the actaul action values:\n",
    "\n",
    "$$\\omega^* = \\arg \\min_{\\omega} \\frac{1}{2N}\\sum_{i=1}^N[Q_\\omega(s_i, a_i) - (r + \\gamma \\max_{a'}Q_\\omega(s'_i, a'))]^2$$\n",
    "\n",
    "Now that we have the loss function, we have expand RL into its NN form. Since DQN use the same idea as of Q-learning, it is also off-policy. Therefore, we can banalnce exploration and exploitation by $\\epsilon$-greedy, and collect the sampled data for later updates. \n",
    "\n",
    "Before we implement DQN, there are 2 modules we need to know that facilitates DQN training to be stable and eminent, namely experience replay and target network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4e826",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Consider a supervised learning task, where we sample 1 or a batch of data for 1 gradient update. With the training goes on, especially when we have multiple epochs, a certain data sample is used multiple times. The reason why we can do this is that in supervised learning, we have an important assumption: the i.i.d assumption that says, all the data in the training set are independent from an identical distrituion. This is also the reason why RL, involving temporal data, could be hard to train using supervised methods: because the temporal struction breaks the i.i.d assumption. \n",
    "\n",
    "Since we use an NN to estimate $Q$, we will need much more data to feed it so that the network is thoroughly trained. This is why we need experience replay. \n",
    "\n",
    "In experience replay, we maintain a replay-loading area, where you will find all the sampled 4-element tuples $(s, a, r, s')$. In the training of the Q-network, we randomly sample from the area. By doing so, the training data satisfies i.i.d assumption, and each data sample can be used multiple times to train the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a337b39",
   "metadata": {},
   "source": [
    "## Target Network\n",
    "The target of DQN training is to let $Q_\\omega(s, a)$ approach $(r + \\gamma \\max_{a'} Q_\\omega(s', a'))$. Since the TD error contains the output of the network, and the output of the network is changing during updates, it is very likely to induce an unstable training. In order to solve this issue, we introduce the target network. \n",
    "\n",
    "The idea of the target network is to fix the Q network for loss calculation. Thus, we have 2 sets of Q-networks:\n",
    "- the previous Q-network for update, responsible for $Q_\\omega(s, a)$ in loss calculation\n",
    "- the target Q-network for stable loss calculation, responsible for $\\max_{a'}Q_{\\omega^-} (s', a')$\n",
    "\n",
    "Every $C$ step the target $Q_{\\omega^-}$ will be synchronized to $Q_\\omega$, while $Q_\\omega$ is updated every step based on gradient updating. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7c4e1",
   "metadata": {},
   "source": [
    "To sum up, DQN algorithm is:\n",
    "\n",
    "- Init $Q_\\omega(s, a)$\n",
    "- Copy $Q_\\omega(s, a)$ to $Q_{\\omega^-}(s, a)$\n",
    "- Init replay pool $R$\n",
    "- for episode $e \\leftarrow 1$ to $E$:\n",
    "    - get init state $s_1$\n",
    "    - for timestep $t \\leftarrow 1$ to $T$:\n",
    "        - use $\\epsilon$-greedy to choose action $a_t$\n",
    "        - take $a_t$ and get the response $r_t, s_{t+1}$\n",
    "        - put $(s_t, a_t, r_t, s_{t+1})$ into $R$\n",
    "        - if $R$ has enough data, sample $N$ data $\\{(s_i, a_i, r_i, s_{i+1})\\}_{i=1, ..., N}$ from $R$\n",
    "        - for each sampled data from $R$, calculate target $y_i = r_i + \\gamma \\max_{a'} Q_{\\omega^-}(s_{i+1}, a')\n",
    "        - minimize loss $L = \\frac{1}{N} \\sum_{i} (y_i - Q_\\omega(s_t, a_t))^2$, then update $Q_\\omega$\n",
    "        - update $Q_{\\omega^-}$\n",
    "    - end for\n",
    "- end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74d6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [torch]m 9/10\u001b[0m [torch]]x]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 setuptools-80.8.0 sympy-1.14.0 torch-2.7.0 typing-extensions-4.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc34061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1354d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691da06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    ''' DQN算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n",
    "                 epsilon, target_update, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = Qnet(state_dim, hidden_dim,\n",
    "                          self.action_dim).to(device)  # Q网络\n",
    "        # 目标网络\n",
    "        self.target_q_net = Qnet(state_dim, hidden_dim,\n",
    "                                 self.action_dim).to(device)\n",
    "        # 使用Adam优化器\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略\n",
    "        self.target_update = target_update  # 目标网络更新频率\n",
    "        self.count = 0  # 计数器,记录更新次数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # epsilon-贪婪策略采取动作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        # implement DQN\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
