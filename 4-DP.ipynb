{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236c5413",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "Now that we have the Bellman Optimality Equation, it is very natural that we come up with the idea to solve MDP with Dynamic Programming (DP). \n",
    "\n",
    "There are 2 main types of DP for RL: policy iteration and value iteration. Policy iteration consists of 2 parts: policy evaluation and policy improvement. The policy evalutaion in policy iteration utilizes the Bellman Expactation Equation to get a policy's state value function, which is a DP process; whereas value iteration directly uses the Bellman Optimality Equation to do DP and reaches the final optimality.\n",
    "\n",
    "Different from Monte Carlo methods, DP methods requires we know $P$ and $r$. Howevver, such white-box environments are rare, which limits the usage of DP. Besides, DP may fits only on descrete MDP, where the states and actions are descrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd490fa7",
   "metadata": {},
   "source": [
    "# Cliff Walking Enverionment\n",
    "\n",
    "This is a classic RL environment, where an agent walks next to a cliff. The agent should avoid the cliff parts and walk from a given starting point to the given end point. Each step has a reward of -1, when the agent entering the cliff area, the reward is -100. Reaching either the end point or the cliff ends the episode. The action space contains 4 actions: up, down, left and right. A action that tries to cross the boundary of the envrionment lead to no move. Below is the code for the cliff walking environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8591d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    \"\"\" 悬崖漫步环境\"\"\"\n",
    "    def __init__(self, ncol=12, nrow=4):\n",
    "        self.ncol = ncol  # 定义网格世界的列\n",
    "        self.nrow = nrow  # 定义网格世界的行\n",
    "        # 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励\n",
    "        self.P = self.createP()\n",
    "\n",
    "    def createP(self):\n",
    "        # 初始化\n",
    "        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n",
    "        # 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                for a in range(4):\n",
    "                    # 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0\n",
    "                    if i == self.nrow - 1 and j > 0:\n",
    "                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]\n",
    "                        continue\n",
    "                    # 其他位置\n",
    "                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n",
    "                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n",
    "                    next_state = next_y * self.ncol + next_x\n",
    "                    reward = -1\n",
    "                    done = False\n",
    "                    # 下一个位置在悬崖或者终点\n",
    "                    if next_y == self.nrow - 1 and next_x > 0:\n",
    "                        done = True\n",
    "                        if next_x != self.ncol - 1:  # 下一个位置在悬崖\n",
    "                            reward = -100\n",
    "                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8c1b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 23, -1, False)]\n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "print(env.P[35][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0846f6b",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "## Policy Evalutaion \n",
    "Policy evaluation evaluates the value of a policy. Recall the Bellman Expectation euqation: $$V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) (r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)V^{\\pi}(s'))$$\n",
    "\n",
    "Using the idea of DP, we can update the value of this iteration by that of the last iteration: $$V^{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) (r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)V^{k}(s'))$$\n",
    "\n",
    "We may choose any starting point $V^0$. According the Bellman Expectation Equation, $V^k=V^\\pi$ is a fixed point of the updating equation above. Hence, when we have $\\max_{s \\in \\mathcal{S}}|V^{k+1}-V^{k}| \\leq \\epsilon$ where $\\epsilon$ is very small, the evalutaion may end.\n",
    "\n",
    "## Policy Improvement\n",
    "Now that we have $V^\\pi$ in Policy Evaluation, we can improve it. Assume we have $Q^\\pi(s, a) > V^\\pi(s)$, we can replace the previous action with action $a$ to improve $V^\\pi(s)$. Similarly, we can assume there exists a policy $\\pi'$, such that $Q^\\pi(s, \\pi'(s)) \\geq V^\\pi(s)$, then we have $V^{\\pi'}(s) \\geq V^\\pi(s)$. This is the policy improvement theorem. Hence, we can improve the policy greedily: $$\\pi'(s) = \\arg \\max_{a \\in \\mathcal{A}} Q^\\pi(s, a) = \\arg \\max_{a \\in \\mathcal{A}} \\{r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)V^{k}(s')\\}$$\n",
    "\n",
    "## Policy Iteration Algotithm\n",
    "Combine policy evaluation and improvement together, we have the algorithm below: \n",
    "\n",
    "- Randomly init $V(s)$, $\\pi(s)$\n",
    "\n",
    "- while $\\Delta > \\theta$, do: # Pollicy Evaluation\n",
    "    - $\\Delta \\leftarrow 0$\n",
    "    - for every $s \\in \\mathcal{S}$:\n",
    "        - $v \\leftarrow V(s)$\n",
    "        - $V(s) \\leftarrow r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, \\pi(s))V(s')$\n",
    "        - $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "    - end for\n",
    "- end while\n",
    "- $\\pi_{old} = \\pi$ # Policy Improvement\n",
    "- for every $s \\in \\mathcal{S}$:\n",
    "    - $\\pi(s) \\leftarrow \\arg \\max_{a \\in \\mathcal{A}} \\{r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)V(s')\\}$\n",
    "- if $\\pi_{old} == \\pi$:\n",
    "    - return $\\pi$\n",
    "- else:\n",
    "    - Redo since the while loop\n",
    "\n",
    "Below is the implementation of Policy Iteration Algorithm in the cliff walking environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3e29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13485ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\" 策略迭代算法 \"\"\"\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.v = [0] * self.env.ncol * self.env.nrow  # 初始化价值为0\n",
    "        self.pi = [[0.25, 0.25, 0.25, 0.25]\n",
    "                   for i in range(self.env.ncol * self.env.nrow)]  # 初始化为均匀随机策略\n",
    "        self.theta = theta  # 策略评估收敛阈值\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "\n",
    "    def policy_evaluation(self):  # 策略评估\n",
    "        cnt = 0  # 计数器\n",
    "        # TODO: fill the blank\n",
    "        while True:\n",
    "            v_new = [0] * self.env.ncol * self.env.nrow\n",
    "            for row in range(self.env.nrow):\n",
    "                for col in range(self.env.ncol):\n",
    "                    a_list = self.pi[row * self.env.ncol + col]\n",
    "                    for a, a_prob in enumerate(a_list):\n",
    "                        for p, next_s, r, done in self.env.P[row * self.env.ncol + col][a]:\n",
    "                            if done:\n",
    "                                v_new[row * self.env.ncol + col] += p * r * a_prob\n",
    "                            else:\n",
    "                                v_new[row * self.env.ncol + col] += p * (r + self.gamma * self.v[next_s]) * a_prob\n",
    "                    \n",
    "            cnt += 1\n",
    "            delta = np.max(np.abs(np.array(v_new) - np.array(self.v)))\n",
    "            # print(f\"delta: {delta}\")\n",
    "            self.v = v_new\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "        print(\"策略评估进行%d轮后完成\" % cnt)\n",
    "\n",
    "    def policy_improvement(self):  # 策略提升\n",
    "        # TODO: fill the blank\n",
    "        for row in range(self.env.nrow):\n",
    "            for col in range(self.env.ncol):\n",
    "                action_vals = []\n",
    "                for a in range(4):\n",
    "                    qsa = 0\n",
    "\n",
    "                    for p, next_s, r, done in self.env.P[row * self.env.ncol + col][a]:\n",
    "                        if done:\n",
    "                            qsa += r * p\n",
    "                        else:\n",
    "                            qsa += (r + self.gamma * self.v[next_s]) * p\n",
    "                    action_vals.append(qsa)\n",
    "\n",
    "                # important: maximize the actions that gives maximal Q(s,a)\n",
    "                maxq = max(action_vals)\n",
    "                cntq = action_vals.count(maxq)\n",
    "\n",
    "                self.pi[row * self.env.ncol + col] = [1 / cntq if q == maxq else 0 for q in action_vals]\n",
    "\n",
    "        print(\"策略提升完成\")\n",
    "        return self.pi\n",
    "\n",
    "    def policy_iteration(self):  # 策略迭代\n",
    "        while 1:\n",
    "            self.policy_evaluation()\n",
    "            old_pi = copy.deepcopy(self.pi)  # 将列表进行深拷贝,方便接下来进行比较\n",
    "            new_pi = self.policy_improvement()\n",
    "            if old_pi == new_pi: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a8cde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略评估进行60轮后完成\n",
      "策略提升完成\n",
      "策略评估进行72轮后完成\n",
      "策略提升完成\n",
      "策略评估进行44轮后完成\n",
      "策略提升完成\n",
      "策略评估进行12轮后完成\n",
      "策略提升完成\n",
      "策略评估进行1轮后完成\n",
      "策略提升完成\n",
      "状态价值：\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n",
      "-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "策略：\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "def print_agent(agent, action_meaning, disaster=[], end=[]):\n",
    "    print(\"状态价值：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 为了输出美观,保持输出6个字符\n",
    "            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')\n",
    "        print()\n",
    "\n",
    "    print(\"策略：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 一些特殊的状态,例如悬崖漫步中的悬崖\n",
    "            if (i * agent.env.ncol + j) in disaster:\n",
    "                print('****', end=' ')\n",
    "            elif (i * agent.env.ncol + j) in end:  # 目标状态\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                a = agent.pi[i * agent.env.ncol + j]\n",
    "                pi_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n",
    "                print(pi_str, end=' ')\n",
    "        print()\n",
    "\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "theta = 0.001\n",
    "gamma = 0.9\n",
    "agent = PolicyIteration(env, theta, gamma)\n",
    "agent.policy_iteration()\n",
    "print_agent(agent, action_meaning, list(range(37, 47)), [47])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465d7d8",
   "metadata": {},
   "source": [
    "# Value Iteration Algorithm\n",
    "The results above shows that the policy evaluation requires many rounds to converge to find the state value function of a policy, which is computation-intensive. Do we have to finish policy evaluation to improve the policy? Actually it is possiblt that even the value function has not converged yet, the policy won't change no matter how the calue function will be updated. \n",
    "\n",
    "The value iteration algorithm conducts 1 round of value updates and directly improve the policy accordingly. Notice that there does not ecist an explicit policy, we only maintain the value state function. \n",
    "\n",
    "To make it more clear, value iteration can be seen as a DP using the Bellman Optimality Equation: $$V^*(s) = \\max_{a \\in \\mathcal{A}}\\{r(s, a) + \\gamma * \\sum_{s'} P(s' |s, a)V^*(s')\\}$$\n",
    "\n",
    "Writing it into iteration-updating format: $$V^{k+1}(s) = \\max_{a \\in \\mathcal{A}}\\{r(s, a) + \\gamma * \\sum_{s'} P(s' |s, a)V^{k}(s')\\}$$\n",
    "\n",
    "Once the iteration reaches the fixed point ($v^{k+1}=v^k$), we can recover the optimal policy by $$\\pi^*(s) = \\arg \\max_a\\{r(s, a) + \\gamma \\sum_{s'} P(s'|s, a)V^*(s')\\}$$\n",
    "\n",
    "The value iteration algorithm is given as:\n",
    "- init $V(s)$ randomly\n",
    "- while $\\Delta > \\theta$ do:\n",
    "    - $\\Delta \\leftarrow 0$\n",
    "    - for each $s \\in \\mathcal{S}$:\n",
    "        - $v \\leftarrow V(s)$\n",
    "        - $V(s) \\leftarrow \\max_{a} r(s, a) + \\gamma \\sum_{s'} P(s'|s, a)V(s')$\n",
    "        - $\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$\n",
    "- end while\n",
    "- return a deterministic policy $\\pi(s) = \\arg \\max_a \\{r(s, a) + \\gamma \\sum_{s'} P(s'|s, a)V(s')\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b72d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值迭代一共进行15轮\n",
      "状态价值：\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n",
      "-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "策略：\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\" 价值迭代算法 \"\"\"\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.v = [0] * self.env.ncol * self.env.nrow  # 初始化价值为0\n",
    "        self.theta = theta  # 价值收敛阈值\n",
    "        self.gamma = gamma\n",
    "        # 价值迭代结束后得到的策略\n",
    "        self.pi = [None for i in range(self.env.ncol * self.env.nrow)]\n",
    "\n",
    "    def value_iteration(self):\n",
    "        cnt = 0\n",
    "        # TODO: fill in the blank\n",
    "        while 1:\n",
    "            v_new = [0] * self.env.ncol * self.env.nrow\n",
    "            for s in range(len(self.v)):\n",
    "                action_vals = []\n",
    "                for a in range(4):\n",
    "                    q = 0\n",
    "                    for p, s_next, r, done in self.env.P[s][a]:\n",
    "                        q += p * (r + self.gamma * self.v[s_next] * (1 - done))\n",
    "                    action_vals.append(q)\n",
    "                v_new[s] = max(action_vals)\n",
    "\n",
    "            delta = np.max(np.abs(np.array(v_new) - np.array(self.v)))\n",
    "            cnt += 1\n",
    "            self.v = v_new\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "        print(\"价值迭代一共进行%d轮\" % cnt)\n",
    "        self.get_policy()\n",
    "\n",
    "    def get_policy(self):  # 根据价值函数导出一个贪婪策略\n",
    "        # TODO: fill in the blank\n",
    "        for s in range(len(self.v)):\n",
    "            action_vals = []\n",
    "            for a in range(4):\n",
    "                q = 0\n",
    "                for p, s_next, r, done in self.env.P[s][a]:\n",
    "                    q += p * (r + self.gamma * self.v[s_next] * (1 - done))\n",
    "                action_vals.append(q)\n",
    "\n",
    "            max_q = max(action_vals)\n",
    "            cnt_q = action_vals.count(max_q)\n",
    "            self.pi[s] = [1 / cnt_q if q == max_q else 0 for q in action_vals]\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "theta = 0.001\n",
    "gamma = 0.9\n",
    "agent = ValueIteration(env, theta, gamma)\n",
    "agent.value_iteration()\n",
    "print_agent(agent, action_meaning, list(range(37, 47)), [47])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027d080",
   "metadata": {},
   "source": [
    "# Frozen Lake Environment\n",
    "The frozen lake enverionment is similar to cliff walk, despite that the action does not deterministically leads to the next state. In the frozen lake environment, the result of an action in a given state is sampled from a distribution of all accessible states given the current state as well as the action. \n",
    "\n",
    "OpenAI Gym provides the frozen lake environment. Every step has reward 0 except the goal with reward 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6c9f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冰洞的索引: {11, 12, 5, 7}\n",
      "目标的索引: {15}\n",
      "[(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "[(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "[(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "[(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")  # 创建环境\n",
    "env = env.unwrapped  # 解封装才能访问状态转移矩阵P\n",
    "env.render()  # 环境渲染,通常是弹窗显示或打印出可视化的环境\n",
    "\n",
    "holes = set()\n",
    "ends = set()\n",
    "for s in env.P:\n",
    "    for a in env.P[s]:\n",
    "        for s_ in env.P[s][a]:\n",
    "            if s_[2] == 1.0:  # 获得奖励为1,代表是目标\n",
    "                ends.add(s_[1])\n",
    "            if s_[3] == True:\n",
    "                holes.add(s_[1])\n",
    "holes = holes - ends\n",
    "print(\"冰洞的索引:\", holes)\n",
    "print(\"目标的索引:\", ends)\n",
    "\n",
    "for a in env.P[14]:  # 查看目标左边一格的状态转移信息\n",
    "    print(env.P[14][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f8f00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略评估进行25轮后完成\n",
      "策略提升完成\n",
      "策略评估进行58轮后完成\n",
      "策略提升完成\n",
      "状态价值：\n",
      " 0.069  0.061  0.074  0.056 \n",
      " 0.092  0.000  0.112  0.000 \n",
      " 0.145  0.247  0.300  0.000 \n",
      " 0.000  0.380  0.639  0.000 \n",
      "策略：\n",
      "<ooo ooo^ <ooo ooo^ \n",
      "<ooo **** <o>o **** \n",
      "ooo^ ovoo <ooo **** \n",
      "**** oo>o ovoo EEEE \n"
     ]
    }
   ],
   "source": [
    "# 这个动作意义是Gym库针对冰湖环境事先规定好的\n",
    "action_meaning = ['<', 'v', '>', '^']\n",
    "theta = 1e-5\n",
    "gamma = 0.9\n",
    "agent = PolicyIteration(env, theta, gamma)\n",
    "agent.policy_iteration()\n",
    "print_agent(agent, action_meaning, [5, 7, 11, 12], [15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03eda30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值迭代一共进行61轮\n",
      "状态价值：\n",
      " 0.069  0.061  0.074  0.056 \n",
      " 0.092  0.000  0.112  0.000 \n",
      " 0.145  0.247  0.300  0.000 \n",
      " 0.000  0.380  0.639  0.000 \n",
      "策略：\n",
      "<ooo ooo^ <ooo ooo^ \n",
      "<ooo **** <o>o **** \n",
      "ooo^ ovoo <ooo **** \n",
      "**** oo>o ovoo EEEE \n"
     ]
    }
   ],
   "source": [
    "action_meaning = ['<', 'v', '>', '^']\n",
    "theta = 1e-5\n",
    "gamma = 0.9\n",
    "agent = ValueIteration(env, theta, gamma)\n",
    "agent.value_iteration()\n",
    "print_agent(agent, action_meaning, [5, 7, 11, 12], [15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f23a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
