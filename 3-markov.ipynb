{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4171379d",
   "metadata": {},
   "source": [
    "# Markov Process\n",
    "\n",
    "## Markov Property\n",
    "In a stochastic process, the state at time $t$ is $s_t \\in \\mathbb{S}$. Usually, $s_t$ only depends on its previous states {$s_0$, $s_1$, $s_2$, ..., $s_{t-1}$}. If further we have $s_t$ only depends on its last previous state $s_{t-1}$, we call this stochastic process to have Markov property, namely $$P(s_t|s_0, s_1, ..., s_{t-1}) = P(s_t|s_{t-1}).$$\n",
    "\n",
    "## Markov Process\n",
    "Markov process represents the stochastic processes with Markov property. Now that we know the current states only depend on a transition distribution between its latest previous states, the whole Markov process can be represented by a tuple $<\\mathcal{S}, \\mathcal{P}>$, where $\\mathcal{S}$ is the set of all possible states, and $\\mathcal{P}$ is a state transition matrix that defines the transition probability between every pair of the states in $\\mathcal{S}$: $$\\mathcal{P} = \\begin{bmatrix}\n",
    "P(s_1|s_1) & \\cdots & P(s_n|s_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P(s_1|s_n) & \\cdots & P(s_n|s_n)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Markov Rewrad Process\n",
    "We may add reward function $r$ and discount factor $\\gamma$ to Markov Process to get a Markov Reward Process $<\\mathcal{S}, \\mathcal{P}, \\mathcal{r}, \\mathcal{\\gamma}>$. $r(s)$ intakes a state $s$ and returns the expectation of reward when the process state transfers into $s$. $\\gamma \\in [0, 1)$ discounts for the uncertainty of further rewards. A $\\gamma$ closer to 1 focuses more on further cumulative profit while closer to 0 cares more about recent rewards.\n",
    "\n",
    "### Return\n",
    "In a Markov Reward Process (MRP), return $G_t$ is the cumulative discounted reward since time $t$ to the end of the process (Notice that we are using $r$ for the expected reward, and $R$ for the acutal reward). $$G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... = \\sum_{k=0}^\\infty \\gamma^{k}R_{t+k}$$\n",
    "\n",
    "Below shows how you may calculate the return in an MPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b5ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：-2.5。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# 定义状态转移概率矩阵P\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
    "gamma = 0.5  # 定义折扣因子\n",
    "\n",
    "\n",
    "# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "\n",
    "# 一个状态序列,s1-s2-s3-s6\n",
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"根据本序列计算得到回报为：%s。\" % G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136450e5",
   "metadata": {},
   "source": [
    "### Value function\n",
    "In an MPR, the expected return of a state is called the value of that state. The values of all states composes the value function $V$. \n",
    "\n",
    "$V(s)=\\mathbb{E}[G_t|S_t=s] \\\\ \\quad = \\mathbb{E}[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... |S_t=s]\\\\ \\quad = \\mathbb{E}[R_t + \\gamma (R_{t+1} + \\gamma R_{t+2} + ... )|S_t=s] \\\\ \\quad = \\mathbb{E}[R_t + \\gamma G_{t+1}|S_t=s] \\\\ \\quad = \\mathbb{E}[R_t|S_t=s] + \\mathbb{E}[\\gamma G_{t+1}|S_t=s] \\\\ \\quad = r(s) + \\gamma \\mathbb{E}[V(S_{t+1})|S_t=s]$.\n",
    "\n",
    "Since the transformation matrix defines how states are transformed, we have:$$V(s) = r(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s)V(s'),$$\n",
    "\n",
    "which is the **Bellman equation** of MRP.\n",
    "\n",
    "Assume we have $n$ states in an MPR, we can write the values for all the states as a column vector $\\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^T$. Similarly, we can have the column vecotr of reward $\\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^T$. Therefore, we got hte matrix format of Bellman equation: $$\\mathcal{V} = \\mathcal{R} + \\gamma \\mathcal{P} \\mathcal{V}$$.\n",
    "\n",
    "Solve the equation, we get the analytical solution of Bellman equation: $$\\mathcal{V} = (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}$$.\n",
    "\n",
    "The above analytical solution have time complexity of $O(n^3)$, so it only fits in small-size MRPs. The following code calculates for the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7d87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP中每个状态价值分别为\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value\n",
    "\n",
    "\n",
    "V = compute(P, rewards, gamma, 6)\n",
    "print(\"MRP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3232fb53",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "MP and MRP are automatic processes. If there exists a stimulus from the environment that works together on the process, we call it Markov Decision Process (MDP). This stimulus is call the action of the agent. Add actions into MRP gives us MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\gamma>$, where $\\mathcal{A}$ is the set of all possible actions. Accordingly, $r$ now refers to $r(s, a)$, a reward that depends on both the state and the action, and $\\mathcal{P}(s'|s,a)$ describes how the action and the current states together points to the next state.\n",
    "\n",
    "### Policy\n",
    "Policy $\\pi$ of a agent describes how the agent take actions. $\\pi(a|s) = P(A_t=a|S_t=s)$ is the probability of the agent taking action $a$ when the current state is $s$. \n",
    "\n",
    "A policy could be deterministic when it has a one-hot distribution, or it could be stochastic where every time the agent samples from the distribution. \n",
    "\n",
    "### State Value Function\n",
    "We use $V^{\\pi}(s)$ to describe the state value at $s$ given the policy $\\pi$ in MDP. $$V^{\\pi}(s) = \\mathbb{E}_{\\pi} [G_t | S_t = s]$$\n",
    "\n",
    "### Action Value Function\n",
    "We use $Q^{\\pi}(s, a)$ to describe the action $a$'s value in state $s$ given policy $\\pi$. $$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [G_t|S_t=s, A_t=a]$$.\n",
    "\n",
    "We can observe that the state salue function is a marginalization of the action value function: $$V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)Q^{\\pi}(s, a)$$\n",
    "\n",
    "Accordingly, we can observe that in MDP, the action value equals the reward of current state-action pair plus the discounted weighted sum of all possible state values weighted by their transformation probability: $$Q^{\\pi}(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a)V(s')$$\n",
    "\n",
    "## Bellman Expectation Equation\n",
    "Simple inference brings us to the Bellman expectation equations:\n",
    "$$V^{\\pi}(s) = \\mathbb{E}_{\\pi} [R_t + \\gamma V^\\pi(S_{t+1})| S_t = s] \\\\ \\quad = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) (r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)V^{\\pi}(s'))$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [R_t + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1})|S_t=s, A_t=a] \\\\ \\quad = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s, a)\\sum_{a \\in \\mathcal{A}} \\pi(a'|s')Q^{\\pi}(s', a')$$\n",
    "\n",
    "## Marginalization of $r$ and $P$\n",
    "Marginalization of $r$ and $P$ gives us a way to transform MDP to MRP. Therefore, we can use the same analytical solution do solve MDP. \n",
    "\n",
    "$$r'(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)r(s, a)$$\n",
    "\n",
    "$$P'(s'|s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)P(s'|s, a)$$\n",
    "\n",
    "The code below shows the analytical solution of our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeaab6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "# 转化后的MRP的状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"MDP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004b5e8",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "Monte Carlo methods estimates by random repetitive draws. Let us see how we can use Monte Carlo methods to estimate the value functions of an MDP. \n",
    "\n",
    "To do so, we can sample several sequences from a state in MDP, and use the average of the returns of all the sequences to be the value estimation, i.e., $V^{\\pi}(s) = \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)}$.\n",
    "\n",
    "Notice that we only need to calculate the values for unqiue sequences, and counts the appearances of each unique sequence in lieu of calcuate per sampling. \n",
    "\n",
    "To further accelerate, we may use the incremental update for the average: $N(s) \\leftarrow N(s) + 1; V(s) \\leftarrow V(s) + \\frac{1}{N(s)} (G_t^{(i)}-V(s))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b66051",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd030ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [(np.str_('s4'), '前往s5', 10, 's5')]\n",
      "第二条序列\n",
      " [(np.str_('s3'), '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n",
      "第五条序列\n",
      " [(np.str_('s4'), '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        # TODO: fill in the logic to sample an episode\n",
    "        timestep = 0 \n",
    "        # sample a state as the staring state\n",
    "        s0 = np.random.choice(S[:-1])\n",
    "        s_cur = s0\n",
    "        while s_cur != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "\n",
    "            # choose an action\n",
    "            rand = np.random.rand()\n",
    "            temp = 0\n",
    "            a = None\n",
    "            for a_prime in A:\n",
    "                temp += Pi.get(join(s_cur, a_prime), 0) # assign prob=0 for all impossible actions\n",
    "                if temp > rand:\n",
    "                    a = a_prime\n",
    "                    r = R.get(join(s_cur, a))\n",
    "                    break\n",
    "            \n",
    "            # get to next state\n",
    "            rand = np.random.rand()\n",
    "            temp = 0\n",
    "            s_next = None\n",
    "            for s_prime in S:\n",
    "                temp += P.get(join(join(s_cur, a), s_prime), 0)\n",
    "                if temp > rand:\n",
    "                    s_next = s_prime\n",
    "                    break\n",
    "            episode.append((s_cur, a, r, s_next))\n",
    "            s_cur = s_next\n",
    "\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# 采样5次,每个序列最长不超过20步\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c174f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.2125020454474666, 's2': -1.6560549844853067, 's3': 0.5462728886332628, 's4': 6.2321596775902135, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "# 对所有采样序列计算所有状态的价值\n",
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        # TODO: calculate for each episode\n",
    "        for s, a, r, s_prime in episode[::-1]: #一个序列从后往前计算\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]\n",
    "\n",
    "\n",
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83f73c",
   "metadata": {},
   "source": [
    "# Occupancy Measure\n",
    "\n",
    "Different policies affects the distribution of actions, thus affecting the distributions of states the agent could access. Therefore, the policy actually affects the value function. To measure such difference, we introduce the occupancy measure.\n",
    "\n",
    "Define the initial state distribution as $\\nu_0(s)$, $P^{\\pi}_t(s)$ as the probability of the agent acces state $s$ at time $t$ under policy $\\pi$, $P^{\\pi}_0(s) = \\nu_0(s)$. Then we can define the state visitation of a policy as $$P^{\\pi}_t(s) = (1-\\gamma) \\sum_{t=0}^\\infty \\gamma^t P^{\\pi}_t(s),$$ where $(1-\\gamma)$ is a normalization factor to make the probability of the summation equals 1.\n",
    "\n",
    "Theoratically, to calculate the state visitation probability, we will need to go through infinty steps. However, it also works for a environment where the agent stops after finite steps (which is usually the case).\n",
    "\n",
    "$P^{\\pi}_t(s)$ has the following property: $$P^{\\pi}_t(s) = (1-\\gamma)\\nu_0(s) + \\gamma \\int P(s'|s,a)\\pi(a|s)\\nu^\\pi(s) ds da$$\n",
    "\n",
    "The occupancy measure $\\rho^\\pi(s, a)$ represents probability of the pair $(s, a)$ being visited: $$\\rho^\\pi(s, a) = (1-\\gamma) \\sum_{t=0}^\\infty \\gamma^t P^{\\pi}_t(s)\\pi(a|s)$$.\n",
    "\n",
    "Furthermore, we have two theorems:\n",
    "\n",
    "Theorem 1: $\\rho^{\\pi_1} = \\rho^{\\pi_1} \\Leftrightarrow \\pi_1 = \\pi_2$.\n",
    "\n",
    "Theorem 2: given a valid occupancy measure $\\rho$, the only policy that has occupancy measure $\\rho$ is $\\pi_\\rho = \\frac{\\rho(s, a)}{\\sum_{a'}\\rho(s, a')}$\n",
    "\n",
    "The following code estimates $\\rho$ by the frequency of $(s, a)$ appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c2aca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11424850994366208 0.239521095567568\n"
     ]
    }
   ],
   "source": [
    "def occupancy(episodes, s, a, timestep_max, gamma):\n",
    "    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n",
    "    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n",
    "    \n",
    "    # TODO: fill up the code to estimate rho\n",
    "    for episode in episodes:\n",
    "        for i, (s_cur, a_cur, r, s_next) in enumerate(episode):\n",
    "            total_times[i] += 1\n",
    "            if s == s_cur and a == a_cur:\n",
    "                occur_times[i] += 1\n",
    "    for i, (tot, occ) in enumerate(zip(total_times, occur_times)):\n",
    "        if occ > 0:\n",
    "            rho += gamma**i * occ / tot\n",
    "\n",
    "\n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad517e",
   "metadata": {},
   "source": [
    "# Optimal Policy\n",
    "\n",
    "The object of RL mostly focuses on finding a policy that makes the agent get the maximal expected return starting from the initial state. To begin with, we difne the order between policies: $\\pi > \\pi' \\iff \\forall s \\in \\mathcal{S}, V^{\\pi}(s) \\geq V^{\\pi'}(s)$. There could be multiple optimal policies, we represent them all as $\\pi*(s)$.\n",
    "\n",
    "According to Theorem 2 in Occupancy Measure, we know all the optimal policies must have the same state value function, which we call the optimal state value function: \n",
    "\n",
    "$V^*(s) = \\max_\\pi V^\\pi(s), \\forall s \\in \\mathcal{S}$. \n",
    "\n",
    "Similarly, we can define the optimal action value function:\n",
    "\n",
    "$Q^*(a, s) = \\max_\\pi Q^{\\pi}(a, s), \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$.\n",
    "\n",
    "To make $Q^\\pi(s, a)$ maximal, it is obvious that we should take the optimal policy at and after the current state, which gives us the relationship between $Q^\\pi(s, a)$ and $V^*(s)$: $$Q^*(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}}P(s'|s, a)V^*(s')$$\n",
    "\n",
    "On the other hand, $$V^*(s) = \\max_{a \\in \\mathcal{A}} Q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cfb5e5",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation\n",
    "If we replace $V^*(s)$ and $Q^*(s, a)$ in the equations above, we get the Bellman optimality quation: $$V^*(s) = \\max_{a \\in \\mathcal{A}}\\{r(s, a)\\ + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a)V^*(s')\\}$$ and $$Q^*(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) \\max_{a' \\in \\mathcal{A}} Q^*(s', a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1f235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
