{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6851f21d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "DP requires a known MDP. Under this situation, the agent does not need to communicate with the env to get information about it. Instead, it can simply use DP to find out the optimal policy and value function. It is similar to a supervised learning task with given data distribution, where we can simply minimize the expected error in lieu of sampling data points.\n",
    "\n",
    "However, this could be inrealistic to most RL scenarios, in which the MDP cannot be written expilicitly and the distribution is unknown. Under this setting, the agent has to interact with the environment and learn from sampled data. Such methods, as there is not enverionment model, are called model-free RL.\n",
    "\n",
    "In this section we will learn 2 classic temporal difference methods that belongs to model-free RL, SARSA and Q-learning. Meanwhile, we also introduce a set of concepts: online learning vs. offline learning. Generally speaking, online learning force the agent to learn from data sampled from its current policy. Once the policy is updated, previous samples are no longer valid. On the other hand, offline learning collects previous sampled data into a replay pool for further utilization. Therefore, offline learning is usually better in utilizing historical data, and has a smaller sample complexity (number of samples required to reach convergence). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a990d",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "Temporal Difference (TD) is a method to evaluate the value function that combines the ideas of Monte Calro (MC) methods and DP. TD is similar to MC in a way that they learn from data samples without the need to know the environment. TD resembles DP by estimating current value functions using later states' values according to the Bellman equation.\n",
    "\n",
    "Review the update of value functions in MC: $$V(s_t) \\leftarrow V(s_t) + \\alpha[G_t - V(s_t)]$$ where we replace $\\frac{1}{N(s)}$ with $\\alpha$, a constant represents the step of the update. MC must wait until the episode ends to calculate $G_t$, while TD can calculate it once a step ends. \n",
    "\n",
    "Instead of taking the expectation of the following values, TD estimates the return of current state by current reward plus next state value (discounted): $$V(s_t) \\leftarrow V(s_t) + \\alpha[r_t + \\gamma V(s_{t+1}) - V(s_t)], $$\n",
    "\n",
    "among which $r_t + \\gamma V(s_{t+1}) - V(s_t)$ is called TD error. \n",
    "\n",
    "Here is why you can replace $G_t$ with $r_t + \\gamma V(s_{t+1})$:\n",
    "\n",
    "$$V_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] \\\\ \\quad = \\mathbb{E}_\\pi[\\sum_{k=0}^\\infty \\gamma^k R_{t+k}| S_t = s] \\\\ \\quad = \\mathbb{E}_\\pi[R_t + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s] \\\\ \\quad = \\mathbb{E}_\\pi[R_t + \\gamma V(s_{t+1}) | S_t = s]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143080f4",
   "metadata": {},
   "source": [
    "# Sarsa Algorithm\n",
    "With our TD estimation, it is nature for us to think if there is a way we can do RL resembling policy iteration. For policy evaluation, we can directly use TD estimation. But what should we do for policy improvement without reward function and state transformation functions? The answer is to use TD estimation for the action value function $Q$: $$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r(s_t, a_t) + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]$$\n",
    "\n",
    "Then we can choose the best action greedily: $\\arg \\max_a(Q(s, a))$. It seems to be a complete algorithm now: use greedy algorithm to interact with the environment, and then use the sampled data from interation to update the state value function by TD estimation. \n",
    "\n",
    "However, there exists something to think before we start our implementation. First, an accurate estimation requires great amount of data samples. But, recall what we do in our value iteration algorithm. Actually we do not need it to be very accurate before we update our policy, the idea of which is call the generalized policy iteration. Second, if we use the greedy selection, some actions may naver appear in our sampled data, in which case we cannot estimate those action values and thus cannot guarantee the updated policy is better than the old one. If I may remind you of what we do in the bandit problem, usually we can replace greedy with $\\epsilon$-greedy. In our case, we have the formula below: $$\\pi(a|s) = \\begin{cases} \\frac{\\epsilon}{|\\mathcal{A}|} + 1 - \\epsilon \\text{, if } a = \\arg \\max_{a' \\in \\mathcal{A}} Q(s, a') \\\\ \\frac{\\epsilon}{|\\mathcal{A}|} \\text{, otherwise}\\end{cases}$$\n",
    "\n",
    "Now, we have the SARSA algorithm. This alogirthm got its name by what its update requires: current state $s$, current action $a$, reward $r$, next state $s'$, and next action $a'$. Below is the full descrition of Sarsa:\n",
    "\n",
    "- Init $Q(s, a)$\n",
    "- for $e \\leftarrow 1$ to $E$ do:\n",
    "    - get start state $s$\n",
    "    - choose the action $a$ by $\\epsilon$-greedy\n",
    "    - for $t \\leftarrow 1$ to $T$ do:\n",
    "        - get $r, s'$ from env\n",
    "        - choose $a'$ by $\\epsilon$-greedy\n",
    "        - $Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)]$\n",
    "        - $s \\leftarrow s'$, $a \\leftarrow a'$\n",
    "    - end for\n",
    "- end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2e82b",
   "metadata": {},
   "source": [
    "Now, let's try Sarsa in cliff walking! Notice we have some different implementation this time to better interact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735383f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # tqdm是显示循环进度条的库\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66053269",
   "metadata": {},
   "source": [
    "Then let's implement Sarsa. We mainly maintain a table $Q_table()$ to store all the action values under current policy. The interaction is done via $\\epsilon$-greedy. Update is done by TD. In default all the actions at the end state becomes 0, which means they will not be updated once initialized as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    \"\"\" Sarsa算法 \"\"\"\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "    def take_action(self, state):  # 选取下一步的操作,具体实现为epsilon-贪婪\n",
    "       # TODO: finish the epsilon-greedy action chosen\n",
    "        rand = np.random.random()\n",
    "        if rand > self.epsilon:\n",
    "            action_vals = [self.Q_table[state, a] for a in range(self.n_action)]\n",
    "            action = np.argmax(action_vals)\n",
    "        else:\n",
    "            action = np.random.choice(range(self.n_action))\n",
    "        return action\n",
    "\n",
    "    def best_action(self, state):  # 用于打印策略\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "\n",
    "    def update(self, s0, a0, r, s1, a1):\n",
    "        # TODO: update Q table\n",
    "        self.Q_table[s0, a0] += self.alpha *(r + self.gamma * self.Q_table[s1, a1] - Q_table[s0, a0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4fcc52",
   "metadata": {},
   "source": [
    "Now let us run Sarsa to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cb5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Q_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m episode_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;66;03m# record the true reward\u001b[39;00m\n\u001b[0;32m     24\u001b[0m next_action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtake_action(next_state)\n\u001b[1;32m---> 25\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     27\u001b[0m action \u001b[38;5;241m=\u001b[39m next_action\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mSarsa.update\u001b[1;34m(self, s0, a0, r, s1, a1)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, s0, a0, r, s1, a1):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# TODO: update Q table\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mQ_table\u001b[49m[s0, a0] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m(r \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m Q_table[s1, a1] \u001b[38;5;241m-\u001b[39m Q_table[s0, a0])\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s1, a1\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Q_table' is not defined"
     ]
    }
   ],
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = Sarsa(ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量\n",
    "\n",
    "return_list = []  # 记录每一条序列的回报\n",
    "for i in range(10):  # 显示10个进度条\n",
    "    # tqdm的进度条功能\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "            episode_return = 0\n",
    "            state = env.reset()\n",
    "            action = agent.take_action(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                # TODO: fill in the blank\n",
    "                episode_return += reward # record the true reward\n",
    "                next_action = agent.take_action(next_state)\n",
    "                agent.update(state, action, reward, next_state, next_action)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            return_list.append(episode_return)\n",
    "            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)\n",
    "\n",
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Sarsa on {}'.format('Cliff Walking'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7d112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
